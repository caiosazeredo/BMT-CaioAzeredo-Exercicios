{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"../RESULT\"\n",
    "resultsStemmerPath = f\"{dataPath}/stemmer/RESULTADOS-STEMMER.csv\"\n",
    "resultsNoStemmerPath = f\"{dataPath}/noStemmer/RESULTADOS-NOSTEMMER.csv\"\n",
    "expectedResultsPath = f\"{dataPath}/stemmer/RESULTADOS_ESPERADOS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsStemmer = pd.read_csv(resultsStemmerPath, sep = \";\").drop(\"rank\", axis = 1)\n",
    "resultsStemmer.columns = [\"queryNumber\", \"documentID\", \"score\"]\n",
    "resultsStemmer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsNoStemmer = pd.read_csv(resultsNoStemmerPath, sep = \";\").drop(\"rank\", axis = 1)\n",
    "resultsNoStemmer.columns = [\"queryNumber\", \"documentID\", \"score\"]\n",
    "resultsNoStemmer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedResults = pd.read_csv(expectedResultsPath, sep = \";\")\n",
    "expectedResults.columns = [\"queryNumber\", \"documentID\", \"relevance\"]\n",
    "expectedResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRetrievedDocs(retrievedDocs: Union[pd.DataFrame, List], limit = None, threshold = None):\n",
    "    if limit is not None and threshold is not None:\n",
    "        raise ValueError(\"Either limit or threshold should be None.\")\n",
    "\n",
    "    allQueryNumbers = retrievedDocs[[\"queryNumber\"]].drop_duplicates()\n",
    "\n",
    "    if limit is not None:\n",
    "        if limit <= 0:\n",
    "            raise ValueError(\"Limit should be greater than zero.\")\n",
    "\n",
    "        else:\n",
    "            if type(retrievedDocs) is list:\n",
    "                retrievedDocs = retrievedDocs[:limit]\n",
    "            else:\n",
    "                tmpDF = retrievedDocs.copy(deep = True)\n",
    "                tmpDF[\"keep\"] = False\n",
    "                tmpDF[\"keep\"] = tmpDF[[\"queryNumber\", \"keep\"]].groupby(\"queryNumber\").transform(\n",
    "                    lambda group: [i < limit for i in range(group.shape[0])]\n",
    "                )\n",
    "                retrievedDocs = tmpDF[tmpDF[\"keep\"]].drop(\"keep\", axis = 1)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        if type(retrievedDocs) is list:\n",
    "            raise ValueError(\"It is not possible to filter retrieved docs by threshold because the scores are not provided.\")\n",
    "        retrievedDocs = retrievedDocs[retrievedDocs[\"score\"] >= threshold]\n",
    "\n",
    "    retrievedDocs = retrievedDocs[[\"queryNumber\", \"documentID\"]]\n",
    "    retrievedDocs = retrievedDocs.groupby(\"queryNumber\").agg({\n",
    "        \"documentID\": lambda group: list(group)\n",
    "    }).reset_index()\n",
    "    retrievedDocs = pd.merge(allQueryNumbers, retrievedDocs, on = \"queryNumber\", how = \"left\")\n",
    "    retrievedDocs[\"documentID\"] = retrievedDocs.documentID.apply(lambda documents: documents if type(documents) == list else [])\n",
    "    \n",
    "    return retrievedDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _precisionScore(queryDocsDF):\n",
    "    retrieved = set(queryDocsDF.retrievedDoc)\n",
    "    relevant = set(queryDocsDF.relevantDoc)\n",
    "    retrievedAndRelevant = retrieved.intersection(relevant)\n",
    "    try:\n",
    "        precision = len(retrievedAndRelevant)/len(retrieved)\n",
    "    except:\n",
    "        precision = np.nan\n",
    "    return precision\n",
    "\n",
    "def _recallScore(queryDocsDF):\n",
    "    retrieved = set(queryDocsDF.retrievedDoc)\n",
    "    relevant = set(queryDocsDF.relevantDoc)\n",
    "    retrievedAndRelevant = retrieved.intersection(relevant)\n",
    "    try:\n",
    "        recall = len(retrievedAndRelevant)/len(relevant)\n",
    "    except:\n",
    "        recall = np.nan\n",
    "    return recall\n",
    "\n",
    "def _f1Score(queryDocsDF):\n",
    "    precision = _precisionScore(queryDocsDF)\n",
    "    recall = _recallScore(queryDocsDF)\n",
    "    try: \n",
    "        f1 = (2*precision*recall)/(precision + recall)\n",
    "    except:\n",
    "        f1 = np.nan\n",
    "    return f1\n",
    "\n",
    "def _rPrecisionScore(queryDocsDF):\n",
    "    relevant = set(queryDocsDF.relevantDoc)\n",
    "    rel = len(relevant)\n",
    "\n",
    "    retrieved = set(queryDocsDF.retrievedDoc[:rel])\n",
    "    retrievedAndRelevant = retrieved.intersection(relevant)\n",
    "    \n",
    "    rPrecision = len(retrievedAndRelevant)/rel\n",
    "    return rPrecision\n",
    "\n",
    "def _meanAveragePrecisionScore(queryDocsDF):\n",
    "    retrieved = queryDocsDF.retrievedDoc\n",
    "    retrievedSet = set(retrieved)\n",
    "    relevant = queryDocsDF.relevantDoc\n",
    "    relevantRanks = [retrieved.index(doc) + 1 if doc in retrievedSet else 0 for doc in relevant]\n",
    "    precisionAtK = []\n",
    "    for k in relevantRanks:\n",
    "        df = queryDocsDF.copy(deep = True)\n",
    "        df[\"retrievedDoc\"] = df[\"retrievedDoc\"][:k]\n",
    "        df[\"relevantDoc\"] = df[\"relevantDoc\"][:k]\n",
    "        precisionAtK.append(_precisionScore(df))\n",
    "    meanAveragePrecision = np.nanmean(precisionAtK)\n",
    "    return meanAveragePrecision\n",
    "\n",
    "def _meanReciprocalRankScore(queryDocsDF):\n",
    "    retrieved = queryDocsDF.retrievedDoc\n",
    "    relevantSet = set(queryDocsDF.relevantDoc)\n",
    "    retrievedAndRelevant = [documentID for documentID in retrieved if documentID in relevantSet]\n",
    "    \n",
    "    if len(retrievedAndRelevant) == 0:\n",
    "        RR = 0\n",
    "    else:\n",
    "        firstRelevantDocumentID = retrievedAndRelevant[0]\n",
    "        firstRelevantRank = retrieved.index(firstRelevantDocumentID) + 1\n",
    "        RR = 1/firstRelevantRank\n",
    "    return RR\n",
    "\n",
    "def _discountedCumulativeGainScore(queryDocsDF):\n",
    "    retrieved = queryDocsDF.retrievedDoc\n",
    "    relevant = {k:v for k, v in zip(queryDocsDF.relevantDoc, queryDocsDF.relevantDocRelevance)}\n",
    "    discountedCumulativeGain = [relevant.get(retrieved[0],0)]\n",
    "    for i, documentID in enumerate(retrieved[1:], start = 2):\n",
    "        discountFactor = 1/np.log2(i)\n",
    "        discountedCumulativeGain.append(discountedCumulativeGain[-1] + relevant.get(documentID, 0)*discountFactor)\n",
    "    return discountedCumulativeGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetricScore(\n",
    "        retrievedDocs: pd.DataFrame, \n",
    "        relevantDocs: pd.DataFrame, \n",
    "        scoreFuncs = [_precisionScore, _recallScore, _f1Score], \n",
    "        queryNumber: int = None, \n",
    "        limit = None, \n",
    "        threshold = None\n",
    "    ):\n",
    "    retrievedDocs = retrievedDocs if queryNumber is None else retrievedDocs[retrievedDocs.queryNumber == queryNumber]       \n",
    "    retrievedDocs = filterRetrievedDocs(retrievedDocs, limit = limit, threshold = threshold)[[\"queryNumber\", \"documentID\"]]\n",
    "\n",
    "    relevantDocs = relevantDocs.groupby(\"queryNumber\").agg({\n",
    "        \"documentID\": lambda group: list(group),\n",
    "        \"relevance\": lambda group: list(group)\n",
    "    }).reset_index()\n",
    "    \n",
    "    queriesDocs = pd.merge(retrievedDocs, relevantDocs, how = \"inner\", on = \"queryNumber\")\n",
    "    queriesDocs.columns = [\"queryNumber\", \"retrievedDoc\", \"relevantDoc\", \"relevantDocRelevance\"]\n",
    "\n",
    "    for scoreFunc in scoreFuncs:\n",
    "        queriesDocs[scoreFunc.__name__] = queriesDocs.apply(lambda row: scoreFunc(row), axis = 1)\n",
    "    return queriesDocs.drop([\"retrievedDoc\", \"relevantDoc\", \"relevantDocRelevance\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotElevenPoints(retrievedDF: pd.DataFrame, relevantDF: pd.DataFrame, limit = None, threshold = None):\n",
    "    scoresDF = getMetricScore(\n",
    "        retrievedDF, \n",
    "        relevantDF, \n",
    "        scoreFuncs = [_precisionScore, _recallScore],\n",
    "        limit = limit,\n",
    "        threshold = threshold\n",
    "    ).sort_values(\"_recallScore\")\n",
    "    scoresDF._recallScore = scoresDF._recallScore.apply(lambda recall: f\"{recall:.2f}\")\n",
    "    scoresDF._precisionScore = scoresDF._precisionScore.apply(lambda precision: f\"{precision:.2f}\")\n",
    "\n",
    "    recallPrecisionTab = pd.crosstab(scoresDF._recallScore, scoresDF.queryNumber)\n",
    "    for row, recall in enumerate(recallPrecisionTab.index):\n",
    "        for column, queryNumber in enumerate(recallPrecisionTab.columns):\n",
    "            try:\n",
    "                precision = scoresDF[(scoresDF.queryNumber == queryNumber) & (scoresDF._recallScore == recall)].iloc[0][\"_precisionScore\"]\n",
    "                recallPrecisionTab.iloc[row, column] = float(precision)\n",
    "            except:\n",
    "                recallPrecisionTab.iloc[row, column] = float(recallPrecisionTab.iloc[row, column])\n",
    "\n",
    "    elevenPointsRecall = [f\"{i/10:.2f}\" for i in range(0,11)]\n",
    "    interpolatedPrecisionColumns = recallPrecisionTab.columns\n",
    "    elevenPointsDF = pd.DataFrame(data = {\"R (%)\": elevenPointsRecall})\n",
    "    elevenPointsDF[interpolatedPrecisionColumns] = 0\n",
    "    elevenPointsDF = elevenPointsDF.set_index(\"R (%)\")\n",
    "\n",
    "    for row, recall in enumerate(elevenPointsDF.index):\n",
    "        for column, queryNumber in enumerate(elevenPointsDF.columns):\n",
    "            interpolatedPrecision = recallPrecisionTab[recallPrecisionTab.index >= recall][queryNumber].max()\n",
    "            elevenPointsDF.iloc[row, column] = interpolatedPrecision\n",
    "\n",
    "    elevenPointsDF[\"averagePrecision\"] = elevenPointsDF.apply(lambda row: f\"{row.mean():.2f}\", axis = 1)\n",
    "    elevenPointsDF = elevenPointsDF.reset_index()[[\"R (%)\", \"averagePrecision\"]]\n",
    "    elevenPointsDF.columns = [\"Recall (%)\", \"Precision (%)\"]\n",
    "    if list(elevenPointsDF[\"Precision (%)\"].value_counts().index) == [\"nan\"]:\n",
    "        raise Exception(\"All queries retrieved 0 documents.\")\n",
    "    elevenPointsDF = elevenPointsDF.apply(lambda column: [int(np.round(float(value)*100)) if value != \"nan\" else 0 for value in column])\n",
    "\n",
    "    fig = sns.lineplot(data = elevenPointsDF, x = \"Recall (%)\", y = \"Precision (%)\")\n",
    "    plt.ylim([0,100])\n",
    "    plt.xlim([0,100])\n",
    "\n",
    "    return elevenPointsDF, fig.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rPrecisionHistogram(resultsA, resultsB, expectedResults):\n",
    "    rPrecisionA = getMetricScore(resultsA, expectedResults, scoreFuncs = [_rPrecisionScore])\n",
    "    rPrecisionB = getMetricScore(resultsB, expectedResults, scoreFuncs = [_rPrecisionScore])\n",
    "    rPrecision = pd.merge(rPrecisionA, rPrecisionB, on = \"queryNumber\", how = \"inner\")\n",
    "    rPrecision[\"delta\"] = rPrecision._rPrecisionScore_x - rPrecision._rPrecisionScore_y\n",
    "    rPrecision = rPrecision[[\"queryNumber\", \"delta\"]]\n",
    "    rPrecision.columns = [\"Query Number\", \"R-Precision A/B\" ]\n",
    "    fig = plt.figure(figsize = (14,5))\n",
    "    fig = sns.barplot(data = rPrecision, x = \"Query Number\", y = \"R-Precision A/B\")\n",
    "    plt.ylim([-1.05, 1.05])\n",
    "    plt.xticks(rotation = 90)\n",
    "    return rPrecision, fig.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAveragePrecision(retrieved, relevant):\n",
    "    mapQueries = getMetricScore(retrieved, relevant, scoreFuncs = [_meanAveragePrecisionScore])\n",
    "    mapSystem = mapQueries._meanAveragePrecisionScore.mean()\n",
    "    return mapSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanReciprocalRank(retrieved, relevant, limit = 10):\n",
    "    queriesRR = getMetricScore(retrieved, relevant, scoreFuncs = [_meanReciprocalRankScore], limit = limit)\n",
    "    return queriesRR._meanReciprocalRankScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discountedCumulativeGain(retrieved, relevant, limit = 20, returnPlot = True):\n",
    "    score = getMetricScore(retrieved, relevant, scoreFuncs = [_discountedCumulativeGainScore], limit = limit)\n",
    "    maxRetrievedDocs = score._discountedCumulativeGainScore.apply(len).max()\n",
    "    scoreVectors = []\n",
    "    for i, vector in enumerate(score._discountedCumulativeGainScore):\n",
    "        scoreVectors.append([])\n",
    "        for j in range(maxRetrievedDocs):\n",
    "            try:\n",
    "                value = vector[j]\n",
    "            except:\n",
    "                value = 0\n",
    "            scoreVectors[i].append(value)\n",
    "    scoreVectors = np.array(scoreVectors)\n",
    "    score = scoreVectors.mean(axis = 0)\n",
    "    if returnPlot:\n",
    "        fig = plt.figure(figsize = (8,6))\n",
    "        fig = sns.lineplot(x = range(1,len(score)+1), y = score)\n",
    "        plt.xlabel(\"Rank\")\n",
    "        plt.ylabel(\"Average Discounted Cumulative Gain\")\n",
    "        return score, fig.get_figure()\n",
    "    else:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizedDiscountedCumulativeGain(retrieved, relevant, limit = 20):\n",
    "    relevances = relevant.set_index([\"queryNumber\", \"documentID\"]).to_dict()['relevance']\n",
    "\n",
    "    idealRetrieval = retrieved.copy(deep = True)\n",
    "    idealRetrieval[\"relevance\"] = pd.Series([relevances.get(pair[1:],0) for pair in idealRetrieval[[\"queryNumber\", \"documentID\"]].itertuples()])\n",
    "    idealRetrieval = idealRetrieval.sort_values([\"queryNumber\", \"relevance\"], ascending = [True, False]).drop(\"relevance\", axis = 1)\n",
    "    idealRetrievalScore = discountedCumulativeGain(idealRetrieval, relevant, limit = limit, returnPlot = False)\n",
    "\n",
    "    retrievedScore = discountedCumulativeGain(retrieved, relevant, limit = limit, returnPlot = False)\n",
    "    \n",
    "    score = retrievedScore / idealRetrievalScore\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Usage examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerScoresAt10 = getMetricScore(resultsStemmer, expectedResults, limit = 10, scoreFuncs = [_precisionScore, _recallScore, _f1Score])\n",
    "stemmerScoresAt10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noStemmerScoresAt10 = getMetricScore(resultsNoStemmer, expectedResults, limit = 10, scoreFuncs = [_precisionScore, _recallScore, _f1Score])\n",
    "noStemmerScoresAt10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rPrecisionDF, rPrecisionFig = rPrecisionHistogram(resultsStemmer, resultsNoStemmer, expectedResults)\n",
    "plt.ylim([-0.15,0.15])\n",
    "rPrecisionFig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerElevenPointsDF, stemmerElevenPointsFig = plotElevenPoints(resultsStemmer, expectedResults, threshold=0.4)\n",
    "stemmerElevenPointsFig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noStemmerElevenPointsDF, noStemmerElevenPointsFig = plotElevenPoints(resultsNoStemmer, expectedResults, threshold=0.4)\n",
    "noStemmerElevenPointsFig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerMAP = meanAveragePrecision(resultsStemmer, expectedResults)\n",
    "stemmerMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noStemmerMAP = meanAveragePrecision(resultsNoStemmer, expectedResults)\n",
    "noStemmerMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerMRR = meanReciprocalRank(resultsStemmer, expectedResults)\n",
    "stemmerMRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerMRR = meanReciprocalRank(resultsNoStemmer, expectedResults)\n",
    "stemmerMRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 20\n",
    "stemmerDCGScores, stemmerDCGFig = discountedCumulativeGain(resultsStemmer, expectedResults, limit = limit)\n",
    "plt.xticks(range(limit+1))\n",
    "plt.ylim([0,15])\n",
    "stemmerDCGFig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 20\n",
    "noStemmerDCGScores, noStemmerDCGFig = discountedCumulativeGain(resultsNoStemmer, expectedResults, limit = limit)\n",
    "plt.xticks(range(limit+1))\n",
    "plt.ylim([0,15])\n",
    "noStemmerDCGFig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerNDCGScores = normalizedDiscountedCumulativeGain(resultsStemmer, expectedResults)\n",
    "stemmerNDCGScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noStemmerNDCGScores = normalizedDiscountedCumulativeGain(resultsNoStemmer, expectedResults)\n",
    "noStemmerNDCGScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
